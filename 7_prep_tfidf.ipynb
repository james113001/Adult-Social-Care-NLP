{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\spedrickcase\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\spedrickcase\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\spedrickcase\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import polars as pl\n",
    "import unicodedata\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_AWS_FUNCTIONS_S3 = os.getenv(\"RUN_AWS_FUNCTIONS_S3\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_AWS_FUNCTIONS_S3 == \"1\":\n",
    "\n",
    "    try:\n",
    "        s3_client = boto3.client(\"s3\", region_name=\"eu-west-2\")\n",
    "        print(\"s3_client is initialized:\", s3_client)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing s3_client: {e}\")\n",
    "        raise e\n",
    "\n",
    "    TMP_DIR = \"/tmp/\"\n",
    "\n",
    "    def download_file_from_s3(bucket_name, key, download_path):\n",
    "        \"\"\"Download a file from S3 to the local filesystem.\"\"\"\n",
    "        s3_client.download_file(bucket_name, key, download_path)\n",
    "        print(f\"Downloaded {key} to {download_path}\")\n",
    "\n",
    "    def upload_file_to_s3(file_path, bucket_name, key):\n",
    "        \"\"\"Upload a file to S3.\"\"\"\n",
    "        s3_client.upload_file(file_path, bucket_name, key)\n",
    "        print(f\"Uploaded {file_path} to {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spedrickcase\\OneDrive - Lambeth Council\\2024\\ASC Project\\asc_predict\\run_2025-04-01_2025-04-07_inference.env\n",
      "_25_04_07\n",
      "25_04_07\n",
      "20250407\n"
     ]
    }
   ],
   "source": [
    "if RUN_AWS_FUNCTIONS_S3 == \"0\":\n",
    "    latest_run_path = 'latest_run.env'\n",
    "else:\n",
    "    latest_run_path = 'latest_run_s3.env'\n",
    "\n",
    "load_dotenv(latest_run_path)\n",
    "\n",
    "# Load the .env file\n",
    "env_file_path = os.getenv(\"env_file_path\") # Update this to your actual path\n",
    "print(env_file_path)\n",
    "load_dotenv(env_file_path)\n",
    "\n",
    "# Access the variables\n",
    "save_file_suffix = os.getenv('save_file_suffix')\n",
    "time_queries_folder = os.getenv('time_queries_folder')\n",
    "topics_outputs_suffix = os.getenv('topics_outputs_suffix')\n",
    "task = os.getenv('task')\n",
    "panel_time_begin = os.getenv('panel_time_begin')\n",
    "panel_time_end = os.getenv('panel_time_end')\n",
    "one_year_before_time_begin = os.getenv('one_year_before_time_begin')\n",
    "two_years_before_time_begin = os.getenv('two_years_before_time_begin')\n",
    "three_years_before_time_begin = os.getenv('three_years_before_time_begin')\n",
    "six_months_before_time_begin = os.getenv('six_months_before_time_begin')\n",
    "six_months_before_time_end = os.getenv('six_months_before_time_end')\n",
    "six_months_after_time_end = os.getenv('six_months_after_time_end')\n",
    "time_begin = os.getenv('time_begin')\n",
    "time_end = os.getenv('time_end')\n",
    "case_note_time_begin = os.getenv('case_note_time_begin')\n",
    "\n",
    "asc_bucket = os.getenv('asc_bucket')\n",
    "asc_folder = os.getenv('asc_folder')\n",
    "training_run_folder = os.getenv('training_run_folder')\n",
    "training_run_tfidf_folder = os.getenv('training_run_tfidf_folder')\n",
    "training_run_modelling_folder = os.getenv('training_run_modelling_folder')\n",
    "training_run_embedding_folder = os.getenv('training_run_embedding_folder')\n",
    "training_run_model_file = os.getenv('training_run_model_file')\n",
    "\n",
    "training_time_period = os.getenv('training_time_period')\n",
    "\n",
    "run_name = os.getenv('run_name')\n",
    "run_base_folder = os.getenv('run_base_folder')\n",
    "relative_file_prefix = os.getenv('relative_file_prefix')\n",
    "queries_sql_folder = os.getenv('queries_sql_folder')\n",
    "relative_queries_sql_folder = os.getenv('relative_queries_sql_folder')\n",
    "\n",
    "case_note_app_load_folder = os.getenv('case_note_app_load_folder')\n",
    "conda_environment_case_note_load = os.getenv('conda_environment_case_note_load')\n",
    "bat_file_location = os.getenv('bat_file_location')\n",
    "\n",
    "topic_model_app_folder = os.getenv('topic_model_app_folder')\n",
    "custom_regex_file_location = os.getenv('custom_regex_file_location')\n",
    "split_sentences_suffix = os.getenv('split_sentences_suffix')\n",
    "\n",
    "output_data_folder = os.getenv('output_data_folder')\n",
    "relative_data_folder = os.getenv('relative_data_folder')\n",
    "output_data_with_query_date_folder = os.getenv('output_data_with_query_date_folder')\n",
    "output_processed_data_folder = os.getenv('output_processed_data_folder')\n",
    "topics_data_folder = os.getenv('topics_data_folder')\n",
    "tfidf_data_folder = os.getenv('tfidf_data_folder')\n",
    "embeddings_folder = os.getenv('embeddings_folder')\n",
    "model_folder = os.getenv('model_folder')\n",
    "\n",
    "# Example usage\n",
    "print(save_file_suffix)\n",
    "print(time_queries_folder)\n",
    "print(topics_outputs_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_processed_data_folder): os.makedirs(output_processed_data_folder)\n",
    "if not os.path.exists(topics_data_folder): os.makedirs(topics_data_folder)\n",
    "if not os.path.exists(tfidf_data_folder): os.makedirs(tfidf_data_folder)\n",
    "if not os.path.exists(embeddings_folder): os.makedirs(embeddings_folder)\n",
    "if not os.path.exists(model_folder): os.makedirs(model_folder)\n",
    "\n",
    "output_file_path_df = pd.read_csv(output_data_with_query_date_folder + \"/output_file_list_df.csv\")\n",
    "\n",
    "def replace_up_to_last_slash(string, replacement):\n",
    "    # Use a regex pattern that matches both '/' and '\\'\n",
    "    string = string.encode('unicode_escape').decode('ascii')\n",
    "    string_split = string.split(\"/\")[1]\n",
    "    out_path = replacement + \"/\" + string_split\n",
    "    return out_path\n",
    "\n",
    "result = output_file_path_df['table_path'].apply(lambda x: replace_up_to_last_slash(x, output_data_with_query_date_folder))\n",
    "\n",
    "output_file_path_df['table_path'] = result\n",
    "\n",
    "output_file_path_df = output_file_path_df.set_index('table_name')['table_path'].to_dict() # Converted to dictionary\n",
    "\n",
    "unique_person_id_list_df = pd.read_csv(output_data_with_query_date_folder + \"/unique_person_id_list.csv\")\n",
    "unique_subgroup_id_list_df = pd.read_csv(output_data_with_query_date_folder + \"/unique_subgroup_id_list.csv\")\n",
    "unique_form_id_list_df = pd.read_csv(output_data_with_query_date_folder + \"/unique_form_id_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_output_folder = tfidf_data_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### https://journals.lww.com/ccejournal/fulltext/2021/06000/impact_of_different_approaches_to_preparing_notes.7.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "# List of words to remove from the stopword set\n",
    "words_to_remove = ['no', 'nor', 'not', 'don', 'don’t', 'wasn', 'wasn’t', 'weren', 'weren’t', \"don't\", \"wasn't\", \"weren't\"]\n",
    "\n",
    "# Remove the specified words from the stopwords set\n",
    "for word in words_to_remove:\n",
    "    stop_words.discard(word.lower())\n",
    "    \n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the small English model (which includes tokenization and lemmatization)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"tok2vec\", \"tagger\", \"attribute_ruler\"])\n",
    "\n",
    "# def clean_text_spacy(text):\n",
    "#     \"\"\"Cleans text by removing HTML tags, extra spaces, and non-alphabetic characters.\"\"\"\n",
    "#     # Remove HTML tags and special characters\n",
    "#     text = re.sub(r'<.*?>|&nbsp;|\\r\\n|&lt;|&gt;|<strong>|</strong>', ' ', text)\n",
    "#     text = text.replace(u'\\xa0', ' ')  # Replace non-breaking space\n",
    "#     return ' '.join(text.split())  # Normalize spaces\n",
    "\n",
    "regex_pattern_1 = r'<.*?>|&nbsp;|\\r\\n|&lt;|&gt;|<strong>|</strong>'\n",
    "regex_pattern_2 = u'\\xa0'\n",
    "\n",
    "\n",
    "def initial_clean(texts, custom_regex=''):\n",
    "\n",
    "    for text in texts:\n",
    "        if not text or pd.isnull(text):\n",
    "            text = \"\"\n",
    "\n",
    "        # Normalize unicode characters to decompose any special forms\n",
    "        normalized_text = unicodedata.normalize('NFKC', text)\n",
    "\n",
    "        # Replace smart quotes and special punctuation with standard ASCII equivalents\n",
    "        replacements = {\n",
    "            '‘': \"'\", '’': \"'\", '“': '\"', '”': '\"', \n",
    "            '–': '-', '—': '-', '…': '...', '•': '*',\n",
    "        }\n",
    "\n",
    "        # Perform replacements\n",
    "        for old_char, new_char in replacements.items():\n",
    "            normalised_text = normalized_text.replace(old_char, new_char)\n",
    "\n",
    "        text = normalised_text\n",
    "\n",
    "    # Convert to polars Series\n",
    "    texts = pl.Series(texts).str.strip_chars()\n",
    "    \n",
    "    # Define a list of patterns and their replacements\n",
    "    patterns = [\n",
    "        (regex_pattern_1, \" \"),\n",
    "        (regex_pattern_2, \" \")\n",
    "    ]\n",
    "    \n",
    "    # Apply each regex replacement\n",
    "    for pattern, replacement in patterns:\n",
    "        texts = texts.str.replace_all(pattern, replacement)\n",
    "    \n",
    "    # Convert the series back to a list\n",
    "    texts = texts.to_list()\n",
    "    \n",
    "    return texts\n",
    "\n",
    "def process_texts_spacy(texts):\n",
    "    \"\"\"Applies cleaning and lemmatization to a batch of texts.\"\"\"\n",
    "    cleaned_texts = initial_clean(texts)\n",
    "\n",
    "    #print(\"pre clean complete\")\n",
    "    #print(nlp.pipeline)\n",
    "    \n",
    "    # Get stopwords set once, outside the loop\n",
    "    stop_words = nlp.Defaults.stop_words\n",
    "    \n",
    "    docs = nlp.pipe(cleaned_texts, batch_size=100, n_process=4)\n",
    "    \n",
    "    #print(\"Text has been lemmatised\")\n",
    "    \n",
    "    processed_texts = []\n",
    "    for doc in docs:\n",
    "        # Using spaCy's vocab for faster lookups\n",
    "        valid_tokens = [token.lemma_ for token in doc if token.is_alpha and token.text not in stop_words]\n",
    "        processed_texts.append(' '.join(valid_tokens))\n",
    "    \n",
    "    return processed_texts\n",
    "\n",
    "def process_data(df, col):\n",
    "    \"\"\"Processes a DataFrame column using spaCy for faster text processing.\"\"\"\n",
    "    df['stemmed_' + col] = process_texts_spacy(df[col].tolist())  # Convert to list for batch processing\n",
    "    return df[['PERSON_ID', 'DECISION_GROUP', 'stemmed_' + col]]\n",
    "\n",
    "\n",
    "def merge_note_form(case_note_df, joined_table, start='GROUP_DECISION_DATE', end=\"GROUP_DECISION_DATE_PLUS_6\", out_col_name=\"case_note\"):\n",
    "    case_note_df['PERSON_ID'] = case_note_df['PERSON_ID'].astype(str)\n",
    "    case_note_df['case_note'] = case_note_df['case_note'].fillna(\"\")\n",
    "\n",
    "    merged_df = pd.merge(case_note_df, joined_table, on=['PERSON_ID'], how='inner')\n",
    "\n",
    "    mask = (merged_df['created_on'] >= merged_df[start]) & \\\n",
    "                    (merged_df['created_on'] < merged_df[end]) & \\\n",
    "                    (merged_df['created_on'] < merged_df['NEXT_DECISION_DATE'])\n",
    "    df_case_notes = (merged_df[mask]\n",
    "                              .groupby(['PERSON_ID', 'DECISION_GROUP', start, end]).agg({\n",
    "                                        \"case_note\": ' '.join}) #'Time period' if we want to split times\n",
    "                              .reset_index())#names=out_col_name))\n",
    "    print(df_case_notes.head())\n",
    "    df_case_notes.rename(columns={\"case_note\":out_col_name}, inplace=True)\n",
    "    df_case_notes.drop(columns=[start,end], axis=1, inplace=True, errors=\"ignore\")\n",
    "    final_df = pd.merge(df_case_notes, joined_table, on=['PERSON_ID','DECISION_GROUP'], how='inner')\n",
    "    #finaldf= finaldf.fillna(\"\")\n",
    "    return final_df\n",
    "\n",
    "def tfidf(df, column, topn, vocab_save_file='', predefined_vocab_file=''):\n",
    "    # If you have a predefined vocabulary, load from file.\n",
    "    if predefined_vocab_file:\n",
    "        top_terms = pd.read_csv(predefined_vocab_file)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "        # Fit and transform the text data\n",
    "        X = vectorizer.fit_transform(df[column])\n",
    "\n",
    "        # Get feature names (terms)\n",
    "        terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # Sum the term frequencies across all documents\n",
    "        term_frequencies = X.toarray().sum(axis=0)\n",
    "        term_freq_df = pd.DataFrame({'term': terms, 'frequency': term_frequencies})\n",
    "        top_terms = term_freq_df.sort_values(by='frequency', ascending=False).head(topn)\n",
    "\n",
    "        # Save vocabulary as a csv file\n",
    "        if vocab_save_file:\n",
    "            top_terms.to_csv(vocab_save_file)\n",
    "    \n",
    "    # Extract the top n terms into a list\n",
    "    top_terms_list = top_terms['term'].tolist()\n",
    "    tfidf_vectorizer = TfidfVectorizer(vocabulary=top_terms_list)\n",
    "\n",
    "    # Check if the column is empty or contains only null values\n",
    "    if df[column].isnull().all() or df[column].eq('').all():\n",
    "        # Create a zero matrix with the same number of features as top_terms_list\n",
    "        X_tfidf = csr_matrix((len(df), len(top_terms_list)))  # Create a sparse matrix\n",
    "    else:\n",
    "        #tfidf_vectorizer = TfidfVectorizer(vocabulary=top_terms_list)\n",
    "        # Fit and transform the text data using TF-IDF\n",
    "        X_tfidf = tfidf_vectorizer.fit_transform(df[column])\n",
    "\n",
    "    # Convert TF-IDF matrix to a DataFrame\n",
    "    tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    return tfidf_df\n",
    "\n",
    "def get_tfidf(df, column, topn=1000, vocab_save_file='tfidf_vocabulary', predefined_vocab_file=''):\n",
    "    '''Wrapper function for tfidf'''\n",
    "    tfidf_df = tfidf(df, column, topn, vocab_save_file, predefined_vocab_file)\n",
    "\n",
    "    # Add the TF-IDF features back to the original DataFrame\n",
    "    df_with_tfidf = pd.concat([df, tfidf_df], axis=1)\n",
    "    df_with_tfidf = df_with_tfidf.drop([column], errors=\"ignore\", axis=1)\n",
    "    return df_with_tfidf\n",
    "\n",
    "def unique_token_count(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
    "    unique_tokens = set(tokens)           # Find unique tokens\n",
    "    return len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joined_table = pd.read_parquet(output_processed_data_folder + \"/joined_table_review_filtered\" + save_file_suffix + \"_six_month_no_added_features.parquet\")\n",
    "joined_table['PERSON_ID'] = joined_table['PERSON_ID'].astype(str)\n",
    "\n",
    "for idx, val in enumerate(joined_table.loc[:,'CHANGE_IN_NEXT_GROUP'].value_counts()):\n",
    "    print(f\"Class={idx}, Count={val}, Percentage={((val/ joined_table.shape[0]) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "case_note_first_three_df = pd.read_csv(tfidf_data_folder + '/filtered_case_notes_first_three_months_f' + save_file_suffix +  '_clean_anon_' + topics_outputs_suffix + '.csv')\n",
    "case_note_three_six_df = pd.read_csv(tfidf_data_folder + '/filtered_case_notes_three_to_six_months_f' + save_file_suffix +  '_clean_anon_' + topics_outputs_suffix + '.csv')\n",
    "case_note_df = pd.concat([case_note_first_three_df,case_note_three_six_df], ignore_index=True).iloc[:,2:]\n",
    "case_note_df['PERSON_ID'] = case_note_df['PERSON_ID'].astype(str)\n",
    "\n",
    "#how_are_you = pd.read_csv(tfidf_data_folder+ '/joined_table_out_panel_how_are_you' + save_file_suffix +  '_clean_anon_' + topics_outputs_suffix + '.csv').iloc[:,2:]\n",
    "#how_are_you['PERSON_ID'] = how_are_you['PERSON_ID'].astype(str)\n",
    "#how_are_you= how_are_you.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how_are_you.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERSON_ID</th>\n",
       "      <th>created_on</th>\n",
       "      <th>note_type</th>\n",
       "      <th>title</th>\n",
       "      <th>case_note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PERSON_ID, created_on, note_type, title, case_note]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_note_df[case_note_df['PERSON_ID'] == '2305']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Update - Home care mobilization telephone disc...\n",
       "1       Requesting SH OD POC with personal care, meals...\n",
       "2       Fikret's parents, who are advancing in age and...\n",
       "3       CARE AND SUPPORT PLACEMENT REVIEW\\n\\nOUTCOMES ...\n",
       "4       What are you requesting?\\n*Long Term x 4 calls...\n",
       "                              ...                        \n",
       "2259    OUTCOMES AND RECOMMENDATIONS:\\n\\n1.For Mr. Sim...\n",
       "2260    Recommendation.\\n\\nCommunity Duty received a r...\n",
       "2261    Reduction in care Monday-Friday as family will...\n",
       "2262                                 No change in service\n",
       "2263    Request to reduce 1:1 hours from 19hrs to 14hr...\n",
       "Name: REQUESTS, Length: 2264, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_table['REQUESTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERSON_ID</th>\n",
       "      <th>created_on</th>\n",
       "      <th>note_type</th>\n",
       "      <th>title</th>\n",
       "      <th>case_note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2375</td>\n",
       "      <td>2025-03-17 15:50:30</td>\n",
       "      <td>PHONE</td>\n",
       "      <td>Unexplained discomfort personal care</td>\n",
       "      <td>Reported to me by staff members that ET has sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2375</td>\n",
       "      <td>2025-01-15 20:40:24</td>\n",
       "      <td>PHONE</td>\n",
       "      <td>Eugenie home late from the Day service</td>\n",
       "      <td>informed  that due to transport bus breaking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2375</td>\n",
       "      <td>2025-02-04 11:13:25</td>\n",
       "      <td>PHONE</td>\n",
       "      <td>Homecare Mobilisation: Telephone call received</td>\n",
       "      <td>Received a call from  (carer) regarding transf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2375</td>\n",
       "      <td>2025-02-17 12:36:50</td>\n",
       "      <td>PHONE</td>\n",
       "      <td>Updated Light touch telephone review conversat...</td>\n",
       "      <td>Updated Light touch telephone review conversat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2375</td>\n",
       "      <td>2025-03-05 14:20:27</td>\n",
       "      <td>PHONE</td>\n",
       "      <td>T/C update with the sister about Eugenie meeti...</td>\n",
       "      <td>I liaised with her sister,  who i informed tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7637</th>\n",
       "      <td>1530517</td>\n",
       "      <td>2024-11-15 15:29:34</td>\n",
       "      <td>PHONE</td>\n",
       "      <td>CDT - T/C received from daughter RE: Issues wi...</td>\n",
       "      <td>CDT - T/C received from daughter reporting iss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7638</th>\n",
       "      <td>1533432</td>\n",
       "      <td>2024-10-29 10:23:07</td>\n",
       "      <td>CSEVENT</td>\n",
       "      <td>email regarding booking review</td>\n",
       "      <td>, Following our conversation , I will be send...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7639</th>\n",
       "      <td>1533432</td>\n",
       "      <td>2024-10-29 10:19:42</td>\n",
       "      <td>CSEVENT</td>\n",
       "      <td>review documents sent</td>\n",
       "      <td>, See attached  s review documents and suppor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7640</th>\n",
       "      <td>1533432</td>\n",
       "      <td>2024-10-29 10:21:39</td>\n",
       "      <td>CSEVENT</td>\n",
       "      <td>review meeting booked</td>\n",
       "      <td>All, The review meeting for  has been booked _</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7641</th>\n",
       "      <td>1533608</td>\n",
       "      <td>2025-01-06 15:55:18</td>\n",
       "      <td>CSEVENT</td>\n",
       "      <td>INTEGRATED BROKERAGE -  response Re request to...</td>\n",
       "      <td>All, Hope all is well and happy . I have task...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7642 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PERSON_ID           created_on note_type  \\\n",
       "0         2375  2025-03-17 15:50:30     PHONE   \n",
       "1         2375  2025-01-15 20:40:24     PHONE   \n",
       "2         2375  2025-02-04 11:13:25     PHONE   \n",
       "3         2375  2025-02-17 12:36:50     PHONE   \n",
       "4         2375  2025-03-05 14:20:27     PHONE   \n",
       "...        ...                  ...       ...   \n",
       "7637   1530517  2024-11-15 15:29:34     PHONE   \n",
       "7638   1533432  2024-10-29 10:23:07   CSEVENT   \n",
       "7639   1533432  2024-10-29 10:19:42   CSEVENT   \n",
       "7640   1533432  2024-10-29 10:21:39   CSEVENT   \n",
       "7641   1533608  2025-01-06 15:55:18   CSEVENT   \n",
       "\n",
       "                                                  title  \\\n",
       "0                  Unexplained discomfort personal care   \n",
       "1               Eugenie home late from the Day service    \n",
       "2        Homecare Mobilisation: Telephone call received   \n",
       "3     Updated Light touch telephone review conversat...   \n",
       "4     T/C update with the sister about Eugenie meeti...   \n",
       "...                                                 ...   \n",
       "7637  CDT - T/C received from daughter RE: Issues wi...   \n",
       "7638                    email regarding booking review    \n",
       "7639                             review documents sent    \n",
       "7640                             review meeting booked    \n",
       "7641  INTEGRATED BROKERAGE -  response Re request to...   \n",
       "\n",
       "                                              case_note  \n",
       "0     Reported to me by staff members that ET has sp...  \n",
       "1      informed  that due to transport bus breaking ...  \n",
       "2     Received a call from  (carer) regarding transf...  \n",
       "3     Updated Light touch telephone review conversat...  \n",
       "4     I liaised with her sister,  who i informed tha...  \n",
       "...                                                 ...  \n",
       "7637  CDT - T/C received from daughter reporting iss...  \n",
       "7638   , Following our conversation , I will be send...  \n",
       "7639   , See attached  s review documents and suppor...  \n",
       "7640     All, The review meeting for  has been booked _  \n",
       "7641   All, Hope all is well and happy . I have task...  \n",
       "\n",
       "[7642 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_note_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF for six months before next review\n",
      "  PERSON_ID  DECISION_GROUP PREDICTION_DATE_LESS_6 PREDICTION_DATE  \\\n",
      "0   1291796               4    2024-10-28 16:28:00      2025-04-07   \n",
      "1   1392476               3    2024-12-05 08:51:00      2025-04-07   \n",
      "2   1394107               3    2024-10-09 00:00:00      2025-04-07   \n",
      "3    146559               4    2025-02-14 16:04:00      2025-04-07   \n",
      "4    205255               2    2024-10-09 00:00:00      2025-04-07   \n",
      "\n",
      "                                           case_note  \n",
      "0   , I hope you are well. The above-named client...  \n",
      "1  Conversation with  about having a spare key fo...  \n",
      "2   sent to  regarding his mother .  Transitionin...  \n",
      "3  Assessor went to  ward on  / / to speak to . O...  \n",
      "4   asked for  email as  (aunt) has received docu...  \n",
      "  PERSON_ID  DECISION_GROUP PREDICTION_DATE_LESS_3 PREDICTION_DATE  \\\n",
      "0   1291796               4    2025-01-07 00:00:00      2025-04-07   \n",
      "1   1392476               3    2025-01-07 00:00:00      2025-04-07   \n",
      "2   1394107               3    2025-01-07 00:00:00      2025-04-07   \n",
      "3    146559               4    2025-02-14 16:04:00      2025-04-07   \n",
      "4    205255               2    2025-01-07 00:00:00      2025-04-07   \n",
      "\n",
      "                                           case_note  \n",
      "0   , I hope you are well. The above-named client...  \n",
      "1  Conversation with  about having a spare key fo...  \n",
      "2   sent to  regarding his mother .  Transitionin...  \n",
      "3  Assessor went to  ward on  / / to speak to . O...  \n",
      "4   asked for  email as  (aunt) has received docu...  \n",
      "  PERSON_ID  DECISION_GROUP PREDICTION_DATE_LESS_6 PREDICTION_DATE_LESS_3  \\\n",
      "0   1392476               3    2024-12-05 08:51:00             2025-01-07   \n",
      "1   1394107               3    2024-10-09 00:00:00             2025-01-07   \n",
      "2    305234               3    2024-10-09 00:00:00             2025-01-07   \n",
      "3    341851               3    2024-10-27 22:13:00             2025-01-07   \n",
      "4    375167               2    2024-10-09 00:00:00             2025-01-07   \n",
      "\n",
      "                                           case_note  \n",
      "0   are not taking any spot placements or clients...  \n",
      "1   phoneline received a call from son requesting...  \n",
      "2  Courtesy call to enquire why  is not attending...  \n",
      "3   contacted and advised that she did not like a...  \n",
      "4  , mother and main carer contacted. Re-intro ma...  \n"
     ]
    }
   ],
   "source": [
    "if training_time_period == \"first_six_months\":\n",
    "    print(\"TFIDF for six months post previous review\")\n",
    "    merged_df_0_6 = merge_note_form(case_note_df, joined_table, 'GROUP_DECISION_DATE', 'GROUP_DECISION_DATE_PLUS_6', 'case_notes_0_6')\n",
    "    merged_df_0_6\n",
    "\n",
    "    merged_df_0_3 = merge_note_form(case_note_df, joined_table, 'GROUP_DECISION_DATE', 'GROUP_DECISION_DATE_PLUS_3', 'case_notes_0_3')\n",
    "    merged_df_0_3\n",
    "\n",
    "    merged_df_3_6 = merge_note_form(case_note_df, joined_table, 'GROUP_DECISION_DATE_PLUS_3', 'GROUP_DECISION_DATE_PLUS_6', 'case_notes_3_6')\n",
    "    merged_df_3_6\n",
    "\n",
    "elif training_time_period == \"latest_six_months\":\n",
    "    print(\"TFIDF for six months before next review\")\n",
    "    merged_df_0_6 = merge_note_form(case_note_df, joined_table, 'PREDICTION_DATE_LESS_6', 'PREDICTION_DATE', 'case_notes_0_6')\n",
    "    merged_df_0_6\n",
    "\n",
    "    merged_df_0_3 = merge_note_form(case_note_df, joined_table, 'PREDICTION_DATE_LESS_3', 'PREDICTION_DATE', 'case_notes_0_3')\n",
    "    merged_df_0_3\n",
    "\n",
    "    merged_df_3_6 = merge_note_form(case_note_df, joined_table, 'PREDICTION_DATE_LESS_6', 'PREDICTION_DATE_LESS_3', 'case_notes_3_6')\n",
    "    merged_df_3_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#merged_df = merged_df_0_6.merge(how_are_you, on=['PERSON_ID', 'DECISION_GROUP'], how='left')\n",
    "#merged_df = merged_df.merge(merged_df_0_3[['PERSON_ID', 'DECISION_GROUP', 'case_notes_0_3']], on=['PERSON_ID', 'DECISION_GROUP'], how='left')\n",
    "#merged_df = merged_df.merge(merged_df_3_6[['PERSON_ID', 'DECISION_GROUP', 'case_notes_0_3']], on=['PERSON_ID', 'DECISION_GROUP'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stemmed_0_6 = process_data(merged_df_0_6, 'case_notes_0_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_0_3 = process_data(merged_df_0_3, 'case_notes_0_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_3_6 = process_data(merged_df_3_6, 'case_notes_3_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmed_how_are_you = process_data(how_are_you, 'GROUP_HOW_ARE_YOU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_table['REQUESTS']= joined_table['REQUESTS'].fillna('')\n",
    "stemmed_requests = process_data(joined_table, 'REQUESTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmed_how_are_you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERSON_ID</th>\n",
       "      <th>DECISION_GROUP</th>\n",
       "      <th>stemmed_REQUESTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10005</td>\n",
       "      <td>2</td>\n",
       "      <td>update home care mobilization telephone discus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1008583</td>\n",
       "      <td>1</td>\n",
       "      <td>requesting sh od poc personal care meals paper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101096</td>\n",
       "      <td>2</td>\n",
       "      <td>fikret parents advancing age experiencing heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10193</td>\n",
       "      <td>2</td>\n",
       "      <td>care and support placement review outcomes and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102240</td>\n",
       "      <td>3</td>\n",
       "      <td>what requesting long term x calls daily dh poc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>95003</td>\n",
       "      <td>1</td>\n",
       "      <td>outcomes and recommendations simms reside curr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>95204</td>\n",
       "      <td>2</td>\n",
       "      <td>recommendation community duty received referra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>97693</td>\n",
       "      <td>2</td>\n",
       "      <td>reduction care monday friday family support we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>98394</td>\n",
       "      <td>3</td>\n",
       "      <td>no change service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>98766</td>\n",
       "      <td>2</td>\n",
       "      <td>request reduce hours week due sporadic engagem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2264 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PERSON_ID  DECISION_GROUP  \\\n",
       "0        10005               2   \n",
       "1      1008583               1   \n",
       "2       101096               2   \n",
       "3        10193               2   \n",
       "4       102240               3   \n",
       "...        ...             ...   \n",
       "2259     95003               1   \n",
       "2260     95204               2   \n",
       "2261     97693               2   \n",
       "2262     98394               3   \n",
       "2263     98766               2   \n",
       "\n",
       "                                       stemmed_REQUESTS  \n",
       "0     update home care mobilization telephone discus...  \n",
       "1     requesting sh od poc personal care meals paper...  \n",
       "2     fikret parents advancing age experiencing heal...  \n",
       "3     care and support placement review outcomes and...  \n",
       "4     what requesting long term x calls daily dh poc...  \n",
       "...                                                 ...  \n",
       "2259  outcomes and recommendations simms reside curr...  \n",
       "2260  recommendation community duty received referra...  \n",
       "2261  reduction care monday friday family support we...  \n",
       "2262                                  no change service  \n",
       "2263  request reduce hours week due sporadic engagem...  \n",
       "\n",
       "[2264 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (task == \"eval\") | (task == \"inference\"):\n",
    "    final_case_note_df = get_tfidf(stemmed_0_6, 'stemmed_case_notes_0_6', 1000, vocab_save_file='', predefined_vocab_file=training_run_tfidf_folder + '/case_note_0_6_tfidf_vocabulary.csv')\n",
    "    final_case_note_0_3_df = get_tfidf(stemmed_0_3, 'stemmed_case_notes_0_3', 1000, vocab_save_file='', predefined_vocab_file=training_run_tfidf_folder + '/case_note_0_3_tfidf_vocabulary.csv')\n",
    "    final_case_note_3_6_df = get_tfidf(stemmed_3_6, 'stemmed_case_notes_3_6', 1000, vocab_save_file='', predefined_vocab_file=training_run_tfidf_folder + '/case_note_3_6_tfidf_vocabulary.csv')\n",
    "\n",
    "    #final_how_are_you_df = get_tfidf(stemmed_how_are_you, 'stemmed_GROUP_HOW_ARE_YOU', 1000, vocab_save_file='', predefined_vocab_file=training_run_tfidf_folder + '/how_are_you_tfidf_vocabulary.csv')\n",
    "    final_requests_df = get_tfidf(stemmed_requests, 'stemmed_REQUESTS', 500, vocab_save_file='', predefined_vocab_file=training_run_tfidf_folder + '/requests_tfidf_vocabulary.csv')\n",
    "\n",
    "elif task == \"training\":\n",
    "    final_case_note_df = get_tfidf(stemmed_0_6, 'stemmed_case_notes_0_6', 1000, vocab_save_file=tfidf_data_folder + '/case_note_0_6_tfidf_vocabulary.csv', predefined_vocab_file='')\n",
    "    final_case_note_0_3_df = get_tfidf(stemmed_0_3, 'stemmed_case_notes_0_3', 1000, vocab_save_file=tfidf_data_folder + '/case_note_0_3_tfidf_vocabulary.csv', predefined_vocab_file='')\n",
    "    final_case_note_3_6_df = get_tfidf(stemmed_3_6, 'stemmed_case_notes_3_6', 1000, vocab_save_file=tfidf_data_folder + '/case_note_3_6_tfidf_vocabulary.csv', predefined_vocab_file='')\n",
    "    \n",
    "    #final_how_are_you_df = get_tfidf(stemmed_how_are_you, 'stemmed_GROUP_HOW_ARE_YOU', 1000, vocab_save_file=tfidf_data_folder + '/how_are_you_tfidf_vocabulary.csv', predefined_vocab_file='')\n",
    "    final_requests_df = get_tfidf(stemmed_requests, 'stemmed_REQUESTS', 500, vocab_save_file=tfidf_data_folder + '/requests_tfidf_vocabulary.csv', predefined_vocab_file='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#final_case_note_df = final_case_note_df.drop(['Stemmed_HowareYou', 'Stemmed_requests'], errors=\"ignore\", axis=1)\n",
    "#final_how_are_you_df = final_how_are_you_df.drop(['Stemmed_CaseNote','Stemmed_requests'], errors=\"ignore\", axis=1)\n",
    "#final_requests_df = final_requests_df.drop(['Stemmed_CaseNote', 'Stemmed_HowareYou'], errors=\"ignore\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#final_case_note_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_case_note_df.to_parquet(tfidf_data_folder + \"/tfidf_case_note_6mo_1000\" + save_file_suffix + \".parquet\", index=None)\n",
    "final_case_note_0_3_df.to_parquet(tfidf_data_folder + \"/tfidf_case_note_0_3mo_1000\" + save_file_suffix + \".parquet\", index=None)\n",
    "final_case_note_3_6_df.to_parquet(tfidf_data_folder + \"/tfidf_case_note_3_6mo_1000\" + save_file_suffix + \".parquet\", index=None)\n",
    "#final_how_are_you_df.to_parquet(tfidf_data_folder + \"/tfidf_how_are_you_1000\" + save_file_suffix + \".parquet\", index=None)\n",
    "final_requests_df.to_parquet(tfidf_data_folder + \"/tfidf_requests_500\" + save_file_suffix + \".parquet\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving files from output folder to relevant folder location\n",
      "function_output_folder: c:\\Users\\spedrickcase\\OneDrive - Lambeth Council\\2024\\ASC Project\\asc_predict\\run\\run_2025-04-01_2025-04-07_inference\\processed_data\\tfidf\n"
     ]
    }
   ],
   "source": [
    "if RUN_AWS_FUNCTIONS_S3 == \"1\":\n",
    "    print(\"Now uploading files from:\", function_output_folder)\n",
    "\n",
    "    # Upload output files back to S3\n",
    "    for root, _, files in os.walk(function_output_folder):\n",
    "        for file_name in files:\n",
    "            print(\"file_name:\", file_name)\n",
    "            local_file_path = os.path.join(root, file_name)\n",
    "            output_key = f\"{tfidf_data_folder}/{file_name}\"\n",
    "            print(\"Output location is:\", output_key)\n",
    "            upload_file_to_s3(local_file_path, asc_bucket, output_key)\n",
    "else:\n",
    "    print(\"Moving files from output folder to relevant folder location\")\n",
    "\n",
    "    print(\"function_output_folder:\", function_output_folder)\n",
    "\n",
    "    # Not necessary to do this locally\n",
    "    # # Move files to relevant folder - \n",
    "    # for root, _, files in os.walk(function_output_folder + \"/\"):\n",
    "    #     for file_name in files:\n",
    "    #         print(\"file_name:\", file_name)\n",
    "    #         local_file_path = os.path.join(root, file_name)\n",
    "    #         destination_file = f\"{function_output_folder}/{file_name}\"\n",
    "    #         print(\"Output location is:\", destination_file)\n",
    "    #         shutil.move(local_file_path, destination_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
